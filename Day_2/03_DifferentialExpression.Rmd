---
title: "Fall Into Bioinformatics"
subtitle: "Sequence alignment"
author: "Jimmy Breen"
date: "17/09/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = FALSE,
    warning = FALSE,
    fig.align = "center",
    results = "hide"
)
if (interactive()) setwd(here::here("Day_1"))
```



```{r setup, echo = FALSE}
knitr::opts_chunk$set(results = "hide", message = FALSE, warning = FALSE)
```


Now that we have our data in a nicely formatted file, we can move to `R` and follow a fairly standard workflow.
Along the way, we'll come across a few useful packages, data structures and coding tricks which will be applicable in many other contexts.

The packages we'll use for this include not only the `tidyverse`, but a few other packages which are hosted on [Bioconductor](www.bioconductor.org), such as `limma`, `edgeR` and `AnnotationHub`.
We'll also add the `magrittr`,`pander` and `scales` packages as they contain some useful additional utilities.

```{r loadPackages}
library(limma)
library(edgeR)
library(AnnotationHub)
library(tidyverse)
library(magrittr)
library(scales)
library(pander)
library(ggrepel)
```

## Data Setup

### Import

First we should import the file we've created using `featureCounts`.

```{r, echo=FALSE}
counts <- read_tsv("../data/genes.out")
```


```{r counts, eval=FALSE}
counts <- read_tsv("../2_alignedData/counts/genes.out")
```

This file has the gene identifiers in the first column, with the remaining columns containing the name of the `bam` file and the number of reads aligning to each gene.
The first thing we might like to do is tidy up those column names.

```{r setColnames}
colnames(counts) <- colnames(counts) %>%
    basename() %>%
    str_remove("_(10|23)_(03|04)_(2014|2016)_S[0-9]_fem_Aligned.+")
```

That looks much cleaner and we haven't lost any important information.

### Create a DGE List

The main object type we like to use for differential gene expression is a `DGEList`, which stands for Digital Gene Expression List.
These objects have two mandatory elements, with the first being our counts and the second being our samples.
In these objects, we can consider the gene IDs to be the row names and the sample names are the column names.
Here's one way to create a `DGEList`.


```{r dgeList}
dgeList <- counts %>%
    as.data.frame() %>%
    column_to_rownames("Geneid") %>%
    DGEList() %>%
    calcNormFactors()
```

In the first 3 lines, we're just setting the gene IDs as the row names instead of being a column.
From there we've turned the remaining columns (i.e. the counts) into a `DGEList` object, and calculated the normalisation factors, which can be seen in `dgeList$samples`
If fitting counts using a negative binomial model (for discrete data), these `norm.factors` are included in the model to adjust for variations in the library size and count distributions.
We'll use a different approach for our analysis today, but doing this as we form the object is still good practice, in case we change tack halfway through our planned analysis.

```{r samples}
dgeList$samples
```

In the `samples` element, we can also set the group variable so let's put our time-points here.

```{r setGroups}
dgeList$samples$group <- colnames(dgeList) %>%
    str_extract("(6|24)mth") %>%
    factor(levels = c("6mth", "24mth"))
```

### Add Gene Information

A common, but optional element that we can include in a `DGEList` is one called `genes`.
This is where we can place information such as the genomic location, the gene symbol and anything else we think is going to be relevant.
First, we need to find this information though, and we'll use the package `AnnotationHub` which contains the *metadata* for all of the annotation packages included in Bioconductor.

```{r ah}
ah <- AnnotationHub()
```

```{r viewAh}
ah
```

This structure is quite advanced, but if you just type in the object name, you'll get an informative summary of all available annotation packages.
Note that we have numerous data providers, species and data classes.
The column on the left is also a shorthand ID we can use to retrieve any of these annotation objects.

We can also subset the Annotation Hub object to help us find what we're looking for.
In the following code, we're restricting our search to zebrafish, with data held sourced from Ensembl.
The final line looks for an object of class `EnsDb` which is a database object containing a large amount of information, but in a relatively user-friendly way.

```{r subsetAh}
ah %>%
    subset(species == "Danio rerio") %>%
    subset(dataprovider == "Ensembl") %>%
    subset(rdataclass == "EnsDb")
```

The last of these objects contains data from release 94, which will be close enough to match the data we have generated so far (which was Ensembl 95).
Ideally, these should be the same, but there is no EnsDb object for Ensembl 95. I' sure one will be made available in the next release of Bioconductor.
Let's use this annotation object and define it as the object `ensDb`

```{r ensDb}
ensDb <- ah[["AH64906"]]
ensDb
```

There are many helper functions for extracting data from this package, such as `transcripts()`, `promoters()` and `genes()`.
We want gene information today, so let's just use that function. 
Note that after we obtain the data, we're using `subset()` to only keep the data from chromosome 2.

```{r genes}
genesGR <- genes(ensDb) %>%
    subset(seqnames == 2)
```

This object is a `GenomicRanges` (or `GRanges`) object and these are the brad & butter of genomic analysis in R. 
We could spend hours just looking at these, but the main point is that on the left of the "pipes" we have the chromosome (`seqnames`) followed by the range of bases the genes is contained within and the strand the gene is located on.
This is the core `GRanges` element, and we could simply return this information using the function `granges()`

```{r granges}
granges(genesGR)
```

Underlying each `GRanges` object is a `seqinfo` object and these contain all of the genomic information about chromosome names and lengths.
If comparing two `GRanges` objects, they must have identical `seqinfo` objects otherwise the comparison will return an error.
This actually makes perfect sense, but can create issues when comparing data from UCSC, NCBI and Ensembl as they all use different formats for their chromosome names, even though they're based on the same assemblies.

```{r seqinfo}
seqinfo(genesGR)
```

To the right of the pipes, we have the metadata columns, accessed using the function `mcols()`.
Notice this returns a `DataFrame` which is a slightly more controlled version of a `data.frame`.
The differences are beyond the scope of this course, but they can easily be coerced to a `data.frame` using `as.data.frame`.
If you want to use the `dplyr` functions on them, you will need to go through this step.

```{r mcols}
mcols(genesGR)
```

When we place this information into our `DGEList` object, some of those columns look a bit redundant, so let's just keep four of the most useful ones.

```{r newMcols}
mcols(genesGR) <- mcols(genesGR)[c("gene_id", "gene_name", "gene_biotype", "entrezid")]
```

If we wish to add this to our `DGEList`, note that the order of genes will be completely different.
To fix this, we just use the `rownames` of our `DGEList` to reorder the `genes` object.
We can also just create the new element `$genes` by simply typing it & providing a value.
`R` will not check that these two objects are compatible, so you have to be on your toes here.

```{r addGenes}
dgeList$genes <- genesGR[rownames(dgeList),]
```

Now when we subset our `DGEList` by gene, the `genes` element will also be subset accordingly and the initial relationships will be maintained.

```{r dge1to4}
dgeList[1:4,]
```


## Data QC

### Undetectable genes

As you may have noticed, no reads aligned to the 4th gene in this dataset so we should really remove it from the data.
There are probably many more genes in this boat too.
Let's do a logical test to see how many genes were not detected in our dataset.
First we'll add up the total counts for every gene and see how many received at least one count.

```{r checkZeroes}
dgeList$counts %>% 
    rowSums() %>%
    is_greater_than(0) %>%
    table()
```

Clearly, a good proportion of our genes were not expressed in our original samples.
A common approach would be to remove undetectable genes using some metric, such as *Counts per Million reads*, known as `cpm`.
We could consider a gene detectable if returning more than 1CPM in every sample from one of the treatment groups.
Although our dataset is small (all libraries are < 1e6 reads), we usually deal with libraries between 20-30million reads, and this would equate to 20-30 reads aligning to a gene, in every sample from a treatment group.
Here our smallest group is 3 so let's see what would happen if we applied that filter.

First we'll calculate the `cpm` for each gene in each sample, and then we'll apply a logical test to each value checking whether it's greater than 1.
Next, we'll add these up for each gene (i.e. row) and this will give us the total number of samples for each gene, that passed our criteria of $cpm > 1$.
Finally, we'll check for genes which passed our criteria in more than 3 samples, as our smallest group is 3. 
We could also have used `is_weakly_greater_than()` which would test for equality ($\geq$) instead of strictly greater than ($>$).

```{r checkFiltering}
dgeList %>%
    cpm() %>%
    is_greater_than(1) %>%
    rowSums() %>%
    is_greater_than(3) %>%
    table()
```

Losing about 1/3 of the genes is pretty common, so let's now apply that to our dataset.
The object `genes2keep` below will be a `logical` vector deciding on whether we keep the gene for downstream analysis, based purely on whether we consider the gene to be detectable.
We'll create a new `DGEList` object by subsetting our primary one.
This way if we change our mind about our filtering strategy, we don't have to rerun all the code above.

```{r dgeFilt}
genes2keep <- dgeList %>%
    cpm() %>%
    is_greater_than(1) %>%
    rowSums() %>%
    is_greater_than(3)
dgeFilt <- dgeList[genes2keep,] %>% calcNormFactors()
```

Let's compare the distributions of the two datasets, using cpm on the log2 scale.
In the following the command `par(mfrow = c(1,2))` is a base graphics approach and sets the plotting `par`ameters to be a `m`ulti=`f`eature layout, with 1 row and 2 columns.
We've done this because the convenient function `plotDensities()` uses base graphics not `ggplot2`.
It's nowhere near as pretty.

```{r plotCPM, fig.cap="Comparison of logCPM distributions before and after filtering for undetectable genes. Values o the x-axis represent logCPM"}
par(mfrow = c(1,2))
dgeList %>%
    cpm(log = TRUE) %>%
    plotDensities(legend = FALSE, main = "A. Before Filtering")
dgeFilt %>%
    cpm(log = TRUE) %>%
    plotDensities(legend = FALSE, main = "B. After Filtering")
par(mfrow = c(1,1))
```

Note the peak at the left in the first plot around zero.
This is all of the genes with near-zero counts.
Then note that this peak is missing the second plot, confirming that we have removed most of the undetectable genes.

### Library Sizes

Next we should check our library sizes.
It does appear that these being prepared on different days has given one of our groups more reads.
This is not ideal but most modelling approaches will be able to handle this.

```{r plotLibSizes, fig.cap = "Library Sizes after filtering for undetectable genes."}
dgeFilt$samples %>%
    ggplot(aes(group, lib.size, fill = group)) +
    geom_boxplot() +
    scale_y_continuous(labels = comma) +
    labs(x = "Timepoint", y = "Library Size") +
    theme_bw() 
```

We've used he function `comma` from the `scales` package here to help us interpret the y-axis.
For most vertebrate datasets, you'd expect >20million reads, but as we've given you a significantly reduced dataset, these numbers are pretty acceptable.

### PCA

Next we might choose to perform a Principal Component Analysis on our data, commonly abbreviated to PCA.
This time, let's take our CPM values & asses them on the log2 scale to make sure our PCA results are not heavily skewed by highly expressed genes.

```{r pca}
pca <- dgeFilt %>%
    cpm(log = TRUE) %>%
    t() %>%
    prcomp() 
```

In our DGEList, we have the genes as the variables of interest for our main analysis, however for the PCA we're looking at out samples as the variables of interest.
The third line in the above code chunk has transposed (`t()`) the matrix returned by `cpm(log = TRUE)` to place the samples as the rows, which is where the function `prcomp()` expects to see the variables of interest.

A quick inspection of the results shows that the first two components capture most of the variability, as expected.
Beyond this observation, the details of PCA are beyond what we can cover here.

```{r summaryPca, results='asis'}
summary(pca)$importance %>% pander(split.tables = Inf)
```

We can also plot our results to see if samples group clearly with the treatment group based on our main two principal components.
Any clear separation can be considered a positive sign that we will find differentially expressed genes.
The following chunk wraps the plotting inside `plotly::ggplotly()` which takes a `ggplot` object and makes it interactive.
You may notice that `label` was included as a plotting aesthetic, but no labels were added as a layer in ggplot2.
These will instead be added once the plot is made interactive.
Interactive plots will only be interactive when compiling an RMarkdown document to an html format, and will remain static if rendering to an MS Word document or pdf.

```{r plotPCA, fig.cap = "PCA showing two clear groups in the data", results='asis'}
plotly::ggplotly(
    pca$x %>%
        as.data.frame() %>%
        rownames_to_column("sample") %>%
        as_tibble() %>%
        dplyr::select(sample, PC1, PC2) %>%
        left_join(rownames_to_column(dgeFilt$samples, "sample")) %>%
        ggplot(aes(PC1, PC2, colour = group, label = sample)) +
        geom_point(size = 3) +
        theme_bw()
)
```

In many workflows you may see the function `plotMDS()` which produces a similar plot to the above.
Multi-Dimensional Scaling can look similar to PCA, but asks a slightly different question of the data.
Usually results found under both methods will reveal similar patterns in your data.
`plotMDS()` doesn't quite give you as pretty a picture as the above either.

## Differential Expression

In the previous sections we have worked with read counts, which are a discrete value and formally cannot be modelled using the assumption of normally distributed data.
This rules out linear models and t-tests for RNA-Seq, so many packages have been developed which use the negative binomial distribution to model these counts (e.g. `edgeR` and `DESeq2`).
An alternative was proposed by [Law et al](https://genomebiology.biomedcentral.com/articles/10.1186/gb-2014-15-2-r29), where they apply a system of weights to the counts which allow the assumption of normality to be applied.
This method is called `voom` and we'll use this today.

```{r voom}
voomData <- voom(dgeFilt)
```

Note that this has added a design matrix to the data based on our `groups` column, and we can use this to perform a simple linear regression on each gene, which amounts to a t-test in this dataset.
From here it's a simple matter to code the analysis and inspect results.

A lot of analysis is performed in the following code chunk:

1. We'll use `lmFit()` to fit the model for every gene using the design matrix in `voomData`. Note that we're not using CPM, but are instead fitting the counts directly, after incorporation of the voom-derived weights.
2. Next we moderate the variances using an empirical Bayes approach. This uses the assumption that our variance estimates will a mix of overestimates and underestimates, and shrinks them all towards a central value. As a strategy, this is widely accepted and has been shown to increase power and reduce false positives.
3. Finally, we'll create a list of all genes with a summary of the results which we then convert to a `tibble`. The last step will remove the gene IDs (i.e. the row names), but because we've included our gene information in the `DGEList` object, this will be added to he results and we'll still know which gene is which.

```{r topTable}
topTable <- voomData %>% 
    lmFit() %>%
    eBayes %>%
    topTable(coef = "group24mth", n = Inf) %>%
    as_tibble()
```

Note that the GRanges information has been coerced into columns to form a `data.frame`/`tibble` and that looks a bit messy.
To tidy things up, the following code will join all of the genomic co-ordinates into a single column, rename a few columns on the fly and by not asking for `ID.width` & `ID.gene_biotype`, we've saved ourselves dealing with a few redundant columns.
We could actually add this to the chunk above to save forming then modifying an `R` object, but it's been left separate for clarity.


```{r editTopTab}
topTable <- topTable %>%
    unite("Range", ID.start, ID.end, sep = "-") %>%
    unite("Location", ID.seqnames, Range, ID.strand, sep = ":") %>%
    dplyr::select(Geneid = ID.gene_id, 
                  Symbol = ID.gene_name,
                  AveExpr, logFC, t, P.Value, 
                  FDR = adj.P.Val, 
                  Location, 
                  Entrez = ID.entrezid)
```

Now that we have our ranked list of genes, we should really inspect these visually.
A commonly used plot is a volcano plot, where we place the logFC estimates on the x-axis, and the position on the y-axis relates to the strength of statistical significance.
Before plotting, we added a simple column called `DE` to indicate whether we considered a gene to be DE, based purely on an FDR-adjusted p-value < 0.05.
We'll use this to colour points.

```{r volcanoPlot, fig.cap="Volcano plot showing DE genes between the two timepoints"}
topTable %>%
    mutate(DE = FDR < 0.05) %>%
    ggplot(aes(logFC, -log10(P.Value), colour = DE)) +
    geom_point(alpha = 0.5) +
    geom_text_repel(data = . %>% 
                        dplyr::filter(DE) %>%
                        dplyr::filter(-log10(P.Value) > 4 | abs(logFC) > 2.5),
                    aes(label = Symbol)) + 
    scale_colour_manual(values = c("grey", "red")) +
    theme_bw() +
    theme(legend.position = "none")
```

As another perspective, we might like to see whether these genes are at the high or low end of the range for expression values.
This is often referred to as an MD plot, which stands for `M`ean expression vs `D`ifference, where difference is more commonly referred to as logFC.
We've added an `arrange()` call in the following to ensure our DE genes are plotted last and are the most visible in our figure.  

```{r plotMD, fig.cap="Mean-Difference plot showing fold-change potted against expression level. Genes considered as DE are highlighted in red."}
topTable %>%
    mutate(DE = FDR < 0.05) %>%
    arrange(desc(P.Value)) %>%
    ggplot(aes(AveExpr, logFC, colour = DE)) +
    geom_point(alpha = 0.5) +
    geom_text_repel(data = . %>% 
                        dplyr::filter(DE) %>%
                        dplyr::filter(abs(logFC) > 2 | AveExpr > 14),
                    aes(label = Symbol)) + 
    scale_colour_manual(values = c("grey", "red")) +
    labs(x = "Average Expression (log2 CPM)",
         y = "log Fold-Change") +
    theme_bw() +
    theme(legend.position = "none")
```

If were happy with our results, we could exprt this list using `write_csv()` or `write_tsv()`.
We could also produce a short table summarising the "big guns" in the dataset.
As there are more than 100 genes considered as DE here, let's restrict to those with logFC beyond the range $\pm1$, which equates to 2-fold up or down-regulation.

```{r printTopTab, results='asis'}
topTable %>%
    dplyr::filter(FDR < 0.05, abs(logFC) > 1) %>%
    dplyr::select(ID = Geneid, Symbol, AveExpr, logFC, P.Value, FDR) %>%
    pander(caption = paste("The", nrow(.), "most DE genes when ranked by p-value, and filtered on a logFC beyond the range $\\pm1$"))
```


