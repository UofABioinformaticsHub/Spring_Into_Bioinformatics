---
title: "Spring Into Bioinformatics"
subtitle: "Intro to NGS"
author: "Mark Armstrong"
date: "01/10/2019"
output: 
  html_document:
    toc: true
---

# Day 3

```{r setup, include=FALSE}
knitr::opts_chunk$set(
    echo = TRUE,
    message = FALSE,
    warning = FALSE,
    fig.align = "center",
    results = "hide"
)
if (interactive()) setwd(here::here("Day_2"))
```
## Welcome

Welcome to Spring Into Bioinformatics for 2019.
Over this 3 day course we'll hopefully cover enough concepts to get you started with your data and analyses.
This course will provide the most benefit if you continue to use the skills in the weeks directly after the course, and is aimed at those with minimal to no prior bioinformatics expertise.
Course material will be available at this URL indefinitely.

Most of the sessions will be self-guided, with key direction provided sporadically at important times.
Please ask as many questions as you need.
The tutors are specifically here to help you understand and develop your skills, so please ensure you take full advantage of their availability.

We strongly encourage you to a) read all of the notes, and b) manually type **all of the code** (unless directed otherwise).
This will provide you with the the most benefit.


### Tutors

This course was primarily written by members of the Bioinformatics Hub and the tutors across the three days will be:

- Nathan Watson-Haigh, Dan Kortschak and Mark Armstrong (Bioinformatics Hub Staff)
- Jimmy Breen (Bioinformatics Hub / SAHMRI)
- Nhi Hin (PhD Candidate, Bioinformatics Hub)
- Melanie Smith (PhD Candidate, Robinson Research Institute)
- Terry Bertozzi (SA Museum)

## Working with NGS Data, Data Quality Checks and Data Alignment

Continuing on with the format of this course this last day is where we are going to learn the initial steps needed to process high-throughput sequencing data. 
In Day 1 we learnt how to analyse and organise data in `R`, use important data processing packages contained in the `tidyverse` ecosystem, and perform plots that you would commonly create for Transcriptome sequencing projects (e.g. Volcano plot, Mean-difference plot etc). 
In Day 2 we learnt how to quantify aligned data and create a gene counts table, run common functions to assess sample and project quality (unsupervised clustering and estimation of library sizes) and perform differential expression analyses.

Next-generation sequencing (NGS) has become an important tool in assessing biological signal within an organism or population.
Stemming from previous technologies that were costly and time-consuming to run, NGS platforms are relatively cheap and enable the investigation of the genome, transcriptome, methylome etc at extremely high resolution.
The high-throughput of these machines also has unique challenges, and it is important that scientists are aware of the potential limitations of the platforms and the issues involved with the production of good quality data.

On this third day, we will introduce the key file types used in genomic analyses,
illustrate the fundamentals of Illumina NGS technology (the current market leader in the production of sequencing data),
describe the affect that library preparation can have on downstream data,
and run through a NGS pipeline from raw sequencing data through to aligning the data against a reference genome and preparing it for the analysis on day 2.

### Today's Schedule

- Session 1: A rundown of NGS concepts and setting up the VM.
  - What is Next Generation Sequencing
  - Effect of library preparation on raw data
  - NGS pipelines in bash

- Session 2: FastQC and Adapter Removal
  - Data quality checks on raw data
  - Interpreting FastQC output
  - Adapter Removal

- Session 3: Quality Check and STAR Alignment
  - Data quality checks on trimmed data
  - Interpreting changes in FastQC output
  - Introduction to STAR
  - Data alignment with STAR

- Session 4: Indexing with SAMtools
  - Introduction to SAMTools
  - Indexing of aligned files
  - Introduction to IGV
  - Viewing aligned and indexed files with IGViewer
  - Wrap up and Close
  
# Next Generation Sequencing

## Introduction to NGS
Before we can begin to analyse any data, it is helpful to understand how it was generated. Whilst there are numerous platforms for generation of NGS data, today we will look at the Illumina Sequencing by Synthesis method, which is one of the most common methods in use today.
Many of you will be familiar with the process involved, but it may be worth looking at the following [5-minute video from Illumina](http://youtu.be/womKfikWlxM).
As setting up the sound with the VMs can be tricky, it will be easier to view this from your own regular browser.
Briefly minimise the VM, open your regular browser & please use your headphones if you brought them.

This video refers to the process tagmentation.
This is a relatively recent method for fragmenting and attaching adapters to DNA, with an alternative, more traditional methods being sonication, poly-adenylation & attachment of appropriate adaptors in separate steps.
This step may vary depending on your experiment, but the important concept to note during sample preparation is that the DNA insert has multiple sequences ligated to either end.
These include 1) the sequencing primers, 2) index & /or barcode sequences, and 3) the flow-cell binding oligos.
To demonstrate these concepts further, observe the following figure that shows the DNA construct needed to run an illumina sequencing run, and the amplification steps required:

![library preparation](../images/libprep.jpg)
  


